{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from input\n",
    "        self.output = np.maximum(0, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        \n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "        \n",
    "        self.output = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333343 0.3333336  0.33333293]\n",
      " [0.3333336  0.3333336  0.33333275]\n",
      " [0.33333376 0.33333382 0.3333324 ]\n",
      " [0.33333397 0.3333341  0.33333194]]\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take\n",
    "# output of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take\n",
    "# output of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Make a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Make a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Make a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "print(activation2.output[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Network Error with Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **loss function**, also referred to as the **cost function**, is the algorithm\n",
    "that quantifies how wrong a model is. **Loss** is the measure of this metric. Since loss is the model’s\n",
    "error, we ideally want it to be 0.\n",
    "\n",
    "You may wonder why we do not calculate the error of a model based on the argmax accuracy.\n",
    "Recall our earlier example of confidence: `[0.22, 0.6, 0.18]` vs `[0.32, 0.36, 0.32]`. If the correct class were indeed the middle one (index 1), the model accuracy would be identical between the two above. But are these two examples really as accurate as each other? They are not, because accuracy is simply applying an argmax to the output to find the index of the biggest value. The output of a neural network is actually confidence, and more confidence in the correct answer is better. Because of this, we strive to increase correct confidence and decrease misplaced confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Cross-Entropy Loss\n",
    "\n",
    "If you’re familiar with linear regression, then you already know one of the loss functions used\n",
    "with neural networks that do regression: **squared error** (or **mean squared error** with neural networks).\n",
    "\n",
    "\n",
    "We’re not performing regression in this example; we’re classifying, so we need a different loss function. The model has a softmax activation function for the output layer, which means it’s outputting a probability distribution. **Categorical cross-entropy** is explicitly used to\n",
    "compare a “ground-truth” probability *(y or “targets”)* and some predicted distribution *(y-hat or “predictions”)*, so it makes sense to use cross-entropy here. It is also one of the most commonly used loss functions with a softmax activation on the output layer.\n",
    "\n",
    "\n",
    "The formula for calculating the categorical cross-entropy of y (actual/desired distribution) and\n",
    "*y-hat* (predicted distribution) is:\n",
    "\n",
    "$$L_i = -\\sum_j y_{i,j} \\log(\\hat{y}_{i,j})$$\n",
    "\n",
    "Where $L_i$ denotes sample loss value, *i* is the i-th sample in the set, *j* is the label/output index, *y* denotes the target values, and *y-hat* denotes the predicted values.\n",
    "\n",
    "Once we start coding  the solution, we'll simplify it further to *-log(correct_class_confidence)*, the formula for which is:\n",
    "\n",
    "$$L_i = - \\log(\\hat{y}_{i,k})$$ \n",
    "\n",
    "where **k** is an index of \"true\" probability\n",
    "\n",
    "Where $L_i$ denotes sample loss value, *i* is the i-th sample in a set, *k* is the index of the target label (ground-true label), *y* denotes the target values and *y-hat* denotes the predicted values.\n",
    "\n",
    "Example - if we take a softmax output of `[0.7, 0.1, 0.2]` and targets of `[1, 0, 0]`, we can apply the calculations as follows:\n",
    "\n",
    "$$L_i = -\\sum_j y_{i,j} \\log(\\hat{y}_{i,j}) = -(1 \\cdot \\log(0.7) + 0 \\cdot log(0.1) + 0 \\cdot \\log(0.2)) = $$\n",
    "\n",
    "$$= - (-0.35667494393873245 + 0 + 0) = 0.35667494393873245$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# An example output from the output layer of the neural network\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "# Ground truth\n",
    "target_output = [1, 0, 0]\n",
    "\n",
    "loss = -(math.log(softmax_output[0])*target_output[0] +\n",
    "         math.log(softmax_output[1])*target_output[1] +\n",
    "         math.log(softmax_output[2])*target_output[2])\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see with one-hot vector targets, or scalar values that represent them, we can make\n",
    "some simple assumptions and use a more basic calculation — what was once an involved formula\n",
    "reduces to the negative log of the target class’ confidence score — the second formula presented at\n",
    "the beginning of this chapter.\n",
    "\n",
    "As we’ve already discussed, the example confidence level might look like `[0.22, 0.6, 0.18]`\n",
    "or `[0.32, 0.36, 0.32]`. In both cases, the *argmax* of these vectors will return the second class\n",
    "as the prediction, but the model’s confidence about these predictions is high only for one of them.\n",
    "The **Categorical Cross-Entropy Loss** accounts for that and outputs a larger loss the lower the\n",
    "confidence is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "-0.05129329438755058\n",
      "-0.10536051565782628\n",
      "-0.2231435513142097\n",
      ". . .\n",
      "-1.6094379124341003\n",
      "-2.3025850929940455\n",
      "-2.995732273553991\n",
      "-4.605170185988091\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(math.log(1.))\n",
    "print(math.log(0.95))\n",
    "print(math.log(0.9))\n",
    "print(math.log(0.8))\n",
    "print('. . .')\n",
    "print(math.log(0.2))\n",
    "print(math.log(0.1))\n",
    "print(math.log(0.05))\n",
    "print(math.log(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve printed different log values for a few example confidences. When the confidence level\n",
    "equals 1, meaning the model is 100% “sure” about its prediction, the loss value for this sample\n",
    "equals 0. The loss value raises with the confidence level, approaching 0. You might also wonder\n",
    "why we did not print the result of log(0) — we’ll explain that shortly.\n",
    "\n",
    "So far, we’ve applied log() to the softmax output, but have neither explained what “log” is\n",
    "nor why we use it. We will save the discussion of “why” until the next chapter, which covers\n",
    "derivatives, gradients, and optimizations; suffice it to say that the log function has some desirable\n",
    "properties. **Log** is short for **logarithm** and is defined as the solution for the x-term in an equation\n",
    "of the form ax = b. For example, 10x = 100 can be solved with a log: log10(100), which evaluates to\n",
    "2. This property of the log function is especially beneficial when e (Euler’s number or ~2.71828)\n",
    "is used in the base (where 10 is in the example). The logarithm with e as its base is referred to as\n",
    "the **natural logarithm**, **natural log**, or simply **log** — you may also see this written as **ln**: ln(x)\n",
    "= log(x) = loge(x) The variety of conventions can make this confusing, so to simplify things,\n",
    "**any mention of log will always be a natural logarithm throughout this book**. The natural log\n",
    "represents the solution for the x-term in the equation ex = b; for example, ex = 5.2 is solved by\n",
    "log(5.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6486586255873816\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "b = 5.2\n",
    "print(np.log(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.199999999999999\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(math.e ** 1.6486586255873816)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a scenario with a neural network that performs classification between three classes, and\n",
    "the neural network classifies in batches of three. After running through the softmax activation\n",
    "function with a batch of 3 samples and 3 classes, the network’s output layer yields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilities for 3 samples\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, there are 3 classes; let’s say\n",
    "we’re trying to classify something as a “dog,” “cat,” or “human.” A dog is class 0 (at index 0), a\n",
    "cat class 1 (index 1), and a human class 2 (index 2). Let’s assume the batch of three sample inputs\n",
    "to this neural network is being mapped to the target values of a dog, cat, and cat. So the targets (as a list of target indices) would be *[0, 1, 1]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_outputs = [[0.7, 0.1, 0.2],\n",
    "                   [0.1, 0.5, 0.4],\n",
    "                   [0.02, 0.9, 0.08]]\n",
    "\n",
    "class_targets = [0, 1, 1]   # dog, cat, cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first value, 0, in `class_targets` means the first softmax output distribution’s intended\n",
    "prediction was the one at the 0th index of *[0.7, 0.1, 0.2]*; the model has a 0.7 confidence\n",
    "score that this observation is a dog. This continues throughout the batch, where the intended target\n",
    "of the 2nd softmax distribution, *[0.1, 0.5, 0.4]*, was at an index of 1; the model only has a\n",
    "0.5 confidence score that this is a cat — the model is less certain about this observation. In the\n",
    "last sample, it’s also the 2nd index from the softmax distribution, a value of 0.9 in this case — a\n",
    "pretty high confidence.\n",
    "\n",
    "With a collection of softmax outputs and their intended targets, we can map these indices to\n",
    "retrieve the values from the softmax distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.5\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = [[0.7, 0.1, 0.2],\n",
    "                   [0.1, 0.5, 0.4],\n",
    "                   [0.02, 0.9, 0.08]]\n",
    "\n",
    "class_targets = [0, 1, 1]\n",
    "\n",
    "for targ_idx, distribution in zip(class_targets, softmax_outputs):\n",
    "    print(distribution[targ_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `zip()` function, again, lets us iterate over multiple iterables at the same time in Python. This\n",
    "can be further simplified using NumPy (we’re creating a NumPy array of the Softmax outputs this\n",
    "time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = [0, 1, 1]\n",
    "\n",
    "print(softmax_outputs[[0, 1, 2], class_targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the 0, 1, and 2 values? NumPy lets us index an array in multiple ways. One of them is to\n",
    "use a list filled with indices and that’s convenient for us — we could use the `class_targets` for\n",
    "this purpose as it already contains the list of indices that we are interested in. The problem is that\n",
    "this has to filter data rows in the array — the second dimension. To perform that, we also need to\n",
    "explicitly filter this array in its first dimension. This dimension contains the predictions and we,\n",
    "of course, want to retain them all. We can achieve that by using a list containing numbers from 0\n",
    "through all of the indices. We know we’re going to have as many indices as distributions in our\n",
    "entire batch, so we can use a `range()` instead of typing each value ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "print(softmax_outputs[\n",
    "    range(len(softmax_outputs)), class_targets\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a list of the confidences at the target indices for each of the samples. Now we apply\n",
    "the negative log to this list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35667494 0.69314718 0.10536052]\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(softmax_outputs[\n",
    "    range(len(softmax_outputs)), class_targets\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want an average loss per batch to have an idea about how our model is doing during\n",
    "training. There are many ways to calculate an average in Python; the most basic form of an\n",
    "average is the **arithmetic mean**: *sum(iterable) / len(iterable)*. NumPy has a method that computes\n",
    "this average on arrays, so we will use that instead. We add NumPy’s average to the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "neg_log = -np.log(softmax_outputs[\n",
    "                range(len(softmax_outputs)), class_targets\n",
    "            ])\n",
    "average_loss = np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already learned that targets can be one-hot encoded, where all values, except for one, are\n",
    "zeros, and the correct label’s position is filled with 1. They can also be sparse, which means that\n",
    "the numbers they contain are the correct class numbers — we are generating them this way with\n",
    "the `spiral_data()` function, and we can allow the loss calculation to accept any of these forms.\n",
    "Since we implemented this to work with sparse labels (as in our training data), we have to add a\n",
    "check if they are one-hot encoded and handle it a bit differently in this new case. The check can\n",
    "be performed by counting the dimensions — if targets are single-dimensional (like a list), they\n",
    "are sparse, but if there are 2 dimensions (like a list of lists), then there is a set of one-hot encoded\n",
    "vectors. In this second case, we’ll implement a solution using the first equation from this chapter,\n",
    "instead of filtering out the confidences at the target labels. We have to multiply confidences by the\n",
    "targets, zeroing out all values except the ones at correct labels, performing a sum along the row\n",
    "axis (axis 1). We have to add a test to the code we just wrote for the number of dimensions, move\n",
    "calculations of the log values outside of this new *if* statement, and implement the solution for the\n",
    "one-hot encoded labels following the first equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = np.array([[1, 0, 0],\n",
    "                          [0, 1, 0],\n",
    "                          [0, 1, 0]])\n",
    "\n",
    "# Probabilities for target values -\n",
    "# only if categorical labels\n",
    "if len(class_targets.shape) == 1:\n",
    "    correct_confidences = softmax_outputs[\n",
    "        range(len(softmax_outputs)),\n",
    "        class_targets\n",
    "    ]\n",
    "\n",
    "# Mask values - only for one-hot encoded labels\n",
    "elif len(class_targets.shape) == 2:\n",
    "    correct_confidences = np.sum(\n",
    "        softmax_outputs*class_targets,\n",
    "        axis=1)\n",
    "    \n",
    "# Losses\n",
    "neg_log = -np.log(correct_confidences)\n",
    "\n",
    "average_loss = np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, there is one additional problem to solve. The softmax output, which is also\n",
    "an input to this loss function, consists of numbers in the range from 0 to 1 - a list of confidences.\n",
    "It is possible that the model will have full confidence for one label making all the remaining\n",
    "confidences zero. Similarly, it is also possible that the model will assign full confidence to a value\n",
    "that wasn’t the target. If we then try to calculate the loss of this confidence of 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29016/3370398627.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  print(-np.log(0))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(-np.log(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we explain this, we need to talk about *log(0)*. From the mathematical point of view, *log(0)*\n",
    "is undefined. We already know the following dependence: if $y=log(x)$, then $e^y=x$. The question of\n",
    "what the resulting *y* is in *y=log(0)* is the same as the question of what’s the *y* in $e^y=0$. In simplified\n",
    "terms, the constant $e$ to any power is always a positive number, and there is no $y$ resulting in $e^y=0$.\n",
    "This means the *log(0)* is undefined. We need to be aware of what the *log(0)* is, and “undefined”\n",
    "does not mean that we don’t know anything about it. Since *log(0)* is undefined, what’s the result\n",
    "for a value very close to 0? We can calculate the limit of a function. How to exactly calculate it\n",
    "exceeds this book, but the solution is:\n",
    "\n",
    "$$\\lim_{x \\to 0^+} \\log(x) = -\\infty$$\n",
    "\n",
    "We read it as the limit of a natural logarithm of x, with x approaching 0 from a positive (it is impossible to calculate the natural logarithm of a negative value) equals negative infinity. What this means is that the limit is negative infinity for an infinitely small x, where x never reaches 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `-np.log(0)` equals `inf`, is it possible to calculate e to the power of negative infinity with Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.e**(-np.inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the result of `inf` for `-np.log(0)` — as much as that makes sense, since the model\n",
    "would be fully wrong, this will be a problem for us to do further calculations with. Later, with\n",
    "optimization, we will also have a problem calculating gradients, starting with a mean value of all\n",
    "sample-wise losses since a single infinite value in a list will cause the average of that list to also\n",
    "be infinite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29016/3055906385.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  print(np.mean([1, 2, 3, -np.log(0)]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.mean([1, 2, 3, -np.log(0)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could add a very small value to the confidence to prevent it from being zero, for example *1e-7*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.11809565095832\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a very small value, one-tenth of a million, to the confidence at its far edge will\n",
    "insignificantly impact the result, but this method yields an additional 2 issues. First, in the case\n",
    "where the confidence value is 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.999999505838704e-08\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(1+1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model is fully correct in a prediction and puts all the confidence in the correct label,\n",
    "loss becomes a negative value instead of being 0. The other problem here is shifting confidence\n",
    "towards 1, even if by a very small value.\n",
    "\n",
    "To prevent both issues, it’s better to clip values from both\n",
    "sides by the same number, 1e-7 in our case. That means that the lowest possible value will become\n",
    "1e-7 (like in the demonstration we just performed) but the highest possible value, instead of being\n",
    "1+1e-7, will become 1-1e-7 (so slightly less than 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000494736474e-07\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(1-1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will prevent loss from being exactly 0, making it a very small value instead, but won’t make\n",
    "it a negative value and won’t bias overall loss towards 1. Within our code and using numpy, we’ll\n",
    "accomplish that using `np.clip()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m y_pred_clipped = np.clip(\u001b[43my_pred\u001b[49m, \u001b[32m1e-7\u001b[39m, \u001b[32m1\u001b[39m - \u001b[32m1e-7\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method can perform clipping on an array of values, so we can apply it to the predictions\n",
    "directly and save this as a separate array, which we’ll use shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the later chapters, we’ll be adding more loss functions and some of the operations that we’ll be\n",
    "performing are common for all of them. One of these operations is how we calculate the overall\n",
    "loss — no matter which loss function we’ll use, the overall loss is always a mean value of all\n",
    "sample losses. Let’s create the Loss class containing the calculate method that will call our\n",
    "loss object’s forward method and calculate the mean value of the returned sample losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In later chapters, we’ll add more code to this class, and the reason for it to exist will become more\n",
    "clear. For now, we’ll use it for this single purpose.\n",
    "\n",
    "Let's convert our loss code into a class for convenience down the line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        # Probabilities for target values\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped*y_true,\n",
    "                axis=1\n",
    "            )\n",
    "        \n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class inherits the `Loss` class and performs all the error calculations that we derived throughout this chapter and can be used as an object. For example, using the manually-created output and targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "loss = loss_function.calculate(softmax_outputs, class_targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While loss is a useful metric for optimizing a model, the metric commonly used in practice along\n",
    "with loss is the **accuracy**, which describes how often the largest confidence is the correct class\n",
    "in terms of a fraction. Conveniently, we can reuse existing variable definitions to calculate the\n",
    "accuracy metric. We will use the *argmax* values from the *softmax outputs* and then compare these\n",
    "to the targets. This is as simple as doing (note that we slightly modified the `softmax_outputs`\n",
    "for the purpose of this example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Probabilities of 3 samples\n",
    "softmax_outputs = np.array([[0.7, 0.2, 0.1],\n",
    "                           [0.5, 0.1, 0.4],\n",
    "                           [0.02, 0.9, 0.08]])\n",
    "# Target (ground-truth) labels for 3 samples\n",
    "class_targets = np.array([0, 1, 1])\n",
    "\n",
    "# Calculate values along second axis (axis of index 1)\n",
    "predictions = np.argmax(softmax_outputs, axis=1)\n",
    "# If targets are one-hot encoded - convert them\n",
    "if len(class_targets.shape) == 2:\n",
    "    class_targets = np.argmax(class_targets, axis=1)\n",
    "# True evaluates to 1; False to 0\n",
    "accuracy = np.mean(predictions == class_targets)\n",
    "\n",
    "print('acc:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also handling one-hot encoded targets by converting them to sparse values using `np.argmax()`.\n",
    "\n",
    "We can add the following line to the end of our full script above to calculate its accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.38333333333333336\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions == y)\n",
    "\n",
    "# Print accuracy\n",
    "print('acc:', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you’ve learned how to perform a forward pass through our network and calculate the\n",
    "metrics to signal if the model is performing poorly, we will embark on optimization in the next\n",
    "chapter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
